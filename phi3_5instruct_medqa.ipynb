{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "zbtx8tKvN55z",
        "2bwXpsrNDvbE",
        "4kpYwX6wDXb0",
        "sAWrisz4DofO"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matan-Vinkler/phi-3.5-finetuned-medqa/blob/main/phi3_5instruct_medqa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning `Phi-3.5-mini-instruct` on Medical QA Dataset"
      ],
      "metadata": {
        "id": "3yCdDlBpFBus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates how to fine-tune the **Phi-3.5-Mini-Instruct** model using **LoRA (Low-Rank Adaptation)** for a domain-specific task: creating a lightweight **medical assistant**.  \n",
        "We use the **Medical Meadow** dataset (instruction-response pairs in the medical domain) and Hugging Face's **TRL `SFTTrainer`** to align the model with concise, factual medical Q&A style outputs.  \n",
        "\n",
        "The pipeline includes:\n",
        "- Loading and formatting the dataset (`instruction → response` format).\n",
        "- Efficient fine-tuning with **LoRA** and **4-bit quantization** (Colab-friendly).\n",
        "- Saving and pushing the fine-tuned model to the Hugging Face Hub.\n",
        "- Running **inference examples** to validate the assistant's responses.\n",
        "\n",
        "⚠️ **Disclaimer**: This model is for **educational purposes only** and is **not a substitute for professional medical advice**.\n"
      ],
      "metadata": {
        "id": "IBahMk0zOjpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Table of Content"
      ],
      "metadata": {
        "id": "zbtx8tKvN55z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Setup](#scrollTo=2bwXpsrNDvbE)\n",
        "\n",
        ">[Data Loading and Preprocessing](#scrollTo=4kpYwX6wDXb0)\n",
        "\n",
        ">[Model Training](#scrollTo=sAWrisz4DofO)\n",
        "\n",
        ">[Model Inference](#scrollTo=IqQdsLAPE8Y8)"
      ],
      "metadata": {
        "id": "KvCeD84lOCaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "2bwXpsrNDvbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll install any dependency that isn't built-in on Colab."
      ],
      "metadata": {
        "id": "ZQM0abgfEMs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"datasets==3.6.0\"\n",
        "!pip install -U bitsandbytes\n",
        "!pip install trl"
      ],
      "metadata": {
        "id": "NIE3CpE9VJhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing all libraries used for this project, and also checking for `datasets` version to be `3.6.0`."
      ],
      "metadata": {
        "id": "FbAd05h4EaVE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TfQ-fCyT1ed"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from datasets import load_dataset, get_dataset_config_names\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "import random\n",
        "\n",
        "datasets.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also login to the HugginFace Hub so we'll be able to upload our model."
      ],
      "metadata": {
        "id": "p0GvgCxPEoYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "sX6fYbNhAm4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "4kpYwX6wDXb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, we'll load the `medalpaca/medical_meadow_medqa` dataset from HugginFace Hub."
      ],
      "metadata": {
        "id": "WOVltbl8Fpg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"medalpaca/medical_meadow_medqa\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "vUAK5XzjUmZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some exploration on the data:"
      ],
      "metadata": {
        "id": "HrbXh_ogF28j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IDX = random.randint(0, len(dataset[\"train\"]))\n",
        "dataset[\"train\"][IDX][\"input\"]"
      ],
      "metadata": {
        "id": "ZnRfi3psZxFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = dataset[\"train\"].select(range(10))\n",
        "sample"
      ],
      "metadata": {
        "id": "fEw_PnMgapSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll define the preprocess function:"
      ],
      "metadata": {
        "id": "bQ-6Q9WYGHVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(sample):\n",
        "    instruction = sample[\"input\"].split(\"? \\n\")[0][2:]\n",
        "    output = sample[\"output\"][3:]\n",
        "\n",
        "    sample[\"text\"] = f\"### Instruction: {instruction}\\n### Response: {output}\"\n",
        "\n",
        "    return sample\n",
        "\n",
        "sample_processed = sample.map(preprocess_data, remove_columns=[\"input\", \"instruction\", \"output\"])\n",
        "sample_processed"
      ],
      "metadata": {
        "id": "HBeeHZgOb22u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_processed[0]"
      ],
      "metadata": {
        "id": "NjcQUqW8cJo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And apply the preprocess function to the whole dataset:"
      ],
      "metadata": {
        "id": "zgR2KUnsGPRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_dataset = dataset[\"train\"].map(preprocess_data, remove_columns=[\"input\", \"instruction\", \"output\"])\n",
        "processed_dataset"
      ],
      "metadata": {
        "id": "CK1xVXOowGtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "sAWrisz4DofO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading model and tokenizer from `microsoft/Phi-3.5-mini-instruct` checkpoint:"
      ],
      "metadata": {
        "id": "1vhVPaNIGaWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "OqkLnWaaf3sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining training configuration and LoRA configuration:"
      ],
      "metadata": {
        "id": "9jmNgwjOGhhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sft_config = SFTConfig(\n",
        "    output_dir=\"./medical-assistant\",\n",
        "    per_device_train_batch_size=1,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=2e-4,\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"Matanvinkler18/phi-3.5-finetuned-medqa\",\n",
        "    bf16=False,\n",
        "    fp16=True,\n",
        "    dataset_text_field=\"text\"\n",
        ")\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        ")"
      ],
      "metadata": {
        "id": "LSUfcZ2Vjh5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we'll create a `Trainer` object and begin to train the model (estimated 3 to 4 hours on T4 GPU on Colab):"
      ],
      "metadata": {
        "id": "GFsfp-zIHVjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=processed_dataset,\n",
        "    peft_config=peft_config,\n",
        "    processing_class=tokenizer,\n",
        "    args=sft_config\n",
        ")"
      ],
      "metadata": {
        "id": "Cq2g9NbYuMab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "wI6Nn3llvF7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Inference"
      ],
      "metadata": {
        "id": "IqQdsLAPE8Y8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll load the model in a `PeftModel` object:"
      ],
      "metadata": {
        "id": "jnXr--0XHwQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model = PeftModel.from_pretrained(model, \"./medical-assistant\")\n",
        "lora_model.eval()"
      ],
      "metadata": {
        "id": "2_DanAR_vLzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And generate a text-generation pipeline with this model and tokenizer:"
      ],
      "metadata": {
        "id": "wDBLptt4H4o7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=lora_model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "ko3iEmxo20FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And test some example medical prompts to see our trained model's performance:"
      ],
      "metadata": {
        "id": "_xNEETCsICsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"\"\"### Instruction:\n",
        "Explain the difference between Type 1 and Type 2 Diabetes.\n",
        "\n",
        "### Response:\"\"\",\n",
        "    \"\"\"### Instruction:\n",
        "List the common symptoms of iron deficiency anemia.\n",
        "\n",
        "### Response:\"\"\",\n",
        "    \"\"\"### Instruction:\n",
        "Explain the difference between bacterial and viral infections.\n",
        "\n",
        "### Response:\"\"\",\n",
        "    \"\"\"### Instruction:\n",
        "A patient complains of chest pain when climbing stairs. Suggest possible causes.\n",
        "\n",
        "### Response:\"\"\",\n",
        "    \"\"\"### Instruction:\n",
        "A patient complains of chest pain when climbing stairs. Suggest possible causes.\n",
        "\n",
        "### Response:\"\"\"\n",
        "          ]\n",
        "\n",
        "for prompt in prompts:\n",
        "    output = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=256,\n",
        "    )\n",
        "\n",
        "    print(output[0][\"generated_text\"])\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "CgGKmNVX26Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z0-brREB9Nz4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}